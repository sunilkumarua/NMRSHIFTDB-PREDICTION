{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1d349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ROOT path set to: C:\\DOCTORAL HUB\\nmr_pipeline_project\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define your project root (adjust if needed)\n",
    "ROOT = Path(r\"C:\\DOCTORAL HUB\\nmr_pipeline_project\")\n",
    "\n",
    "# Quick check\n",
    "print(f\" ROOT path set to: {ROOT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44b6119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Data Loaded Successfully"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecules: 61,215\n",
      "Assignments: 678,775\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>inchi</th>\n",
       "      <th>solvent</th>\n",
       "      <th>temperature_k</th>\n",
       "      <th>has_13c</th>\n",
       "      <th>has_1h</th>\n",
       "      <th>n_atoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C1(C(C(C2(C(C1([H])[H])(C(C(=C(C2([H])[H])[H])...</td>\n",
       "      <td>InChI=1S/C15H22O3/c1-13(2)7-4-8-14(3)12(13)6-5...</td>\n",
       "      <td>CDCl3</td>\n",
       "      <td>298.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Subergorgiol</td>\n",
       "      <td>C1(C(C2(C3(C1(C(=C(C3(C([H])([H])[H])[H])C(O[H...</td>\n",
       "      <td>InChI=1S/C15H24O/c1-10-4-7-15-11(2)12(9-16)8-1...</td>\n",
       "      <td>CDCl3</td>\n",
       "      <td>298.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mol_idx          name                                             smiles  \\\n",
       "0        1           NaN  C1(C(C(C2(C(C1([H])[H])(C(C(=C(C2([H])[H])[H])...   \n",
       "1        2  Subergorgiol  C1(C(C2(C3(C1(C(=C(C3(C([H])([H])[H])[H])C(O[H...   \n",
       "\n",
       "                                               inchi solvent  temperature_k  \\\n",
       "0  InChI=1S/C15H22O3/c1-13(2)7-4-8-14(3)12(13)6-5...   CDCl3          298.0   \n",
       "1  InChI=1S/C15H24O/c1-10-4-7-15-11(2)12(9-16)8-1...   CDCl3          298.0   \n",
       "\n",
       "   has_13c  has_1h  n_atoms  \n",
       "0     True   False       40  \n",
       "1     True   False       40  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>element</th>\n",
       "      <th>shift_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>C</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>18.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mol_idx  atom_index element  shift_ppm\n",
       "0        1          11       C       17.6\n",
       "1        1           1       C       18.3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "# --- Define paths ---\n",
    "ROOT = Path(\"C:/DOCTORAL HUB/nmr_pipeline_project\")\n",
    "VERIFIED = ROOT / \"data\" / \"curated\" / \"verified\"\n",
    "DESC = ROOT / \"data\" / \"descriptors\"\n",
    "MERGED = ROOT / \"data\" / \"merged\"\n",
    "REPORTS = ROOT / \"data\" / \"reports\"\n",
    "\n",
    "MERGED.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load verified datasets ---\n",
    "mols = pd.read_csv(VERIFIED / \"molecules_verified.csv\")\n",
    "assigns = pd.read_csv(VERIFIED / \"assignments_verified.csv\")\n",
    "\n",
    "display(Markdown(\"## Data Loaded Successfully\"))\n",
    "print(f\"Molecules: {len(mols):,}\")\n",
    "print(f\"Assignments: {len(assigns):,}\")\n",
    "display(mols.head(2))\n",
    "display(assigns.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176d88a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Descriptors Loaded"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECFP0: 60,792 rows\n",
      "ECFP2: 60,792 rows\n",
      "ECFP4: 60,792 rows\n",
      "HOSE: 513,880 rows\n"
     ]
    }
   ],
   "source": [
    "ecfp0 = pd.read_csv(DESC / \"ecfp0.csv\")\n",
    "ecfp2 = pd.read_csv(DESC / \"ecfp2.csv\")\n",
    "ecfp4 = pd.read_csv(DESC / \"ecfp4.csv\")\n",
    "hose = pd.read_csv(DESC / \"hose.csv\")\n",
    "\n",
    "display(Markdown(\"## Descriptors Loaded\"))\n",
    "print(f\"ECFP0: {len(ecfp0):,} rows\")\n",
    "print(f\"ECFP2: {len(ecfp2):,} rows\")\n",
    "print(f\"ECFP4: {len(ecfp4):,} rows\")\n",
    "print(f\"HOSE: {len(hose):,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58e7bc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "##  Validation of Index Keys"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Common molecule indices across all datasets: 60,792\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"##  Validation of Index Keys\"))\n",
    "\n",
    "common_idx = set(mols['mol_idx']).intersection(assigns['mol_idx']).intersection(ecfp0['mol_idx'])\n",
    "print(f\" Common molecule indices across all datasets: {len(common_idx):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd26bebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Merging ECFP Descriptors"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ECFP merged shape: (60792, 3073)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mol_idx</th>\n",
       "      <th>bit_0_r0</th>\n",
       "      <th>bit_1_r0</th>\n",
       "      <th>bit_2_r0</th>\n",
       "      <th>bit_3_r0</th>\n",
       "      <th>bit_4_r0</th>\n",
       "      <th>bit_5_r0</th>\n",
       "      <th>bit_6_r0</th>\n",
       "      <th>bit_7_r0</th>\n",
       "      <th>bit_8_r0</th>\n",
       "      <th>...</th>\n",
       "      <th>bit_1014</th>\n",
       "      <th>bit_1015</th>\n",
       "      <th>bit_1016</th>\n",
       "      <th>bit_1017</th>\n",
       "      <th>bit_1018</th>\n",
       "      <th>bit_1019</th>\n",
       "      <th>bit_1020</th>\n",
       "      <th>bit_1021</th>\n",
       "      <th>bit_1022</th>\n",
       "      <th>bit_1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mol_idx  bit_0_r0  bit_1_r0  bit_2_r0  bit_3_r0  bit_4_r0  bit_5_r0  \\\n",
       "0        1         0         0         0         0         0         0   \n",
       "1        2         0         0         0         0         0         0   \n",
       "2        3         0         0         0         0         0         0   \n",
       "\n",
       "   bit_6_r0  bit_7_r0  bit_8_r0  ...  bit_1014  bit_1015  bit_1016  bit_1017  \\\n",
       "0         0         0         0  ...         0         0         0         0   \n",
       "1         0         0         0  ...         0         0         0         0   \n",
       "2         0         0         0  ...         0         0         0         0   \n",
       "\n",
       "   bit_1018  bit_1019  bit_1020  bit_1021  bit_1022  bit_1023  \n",
       "0         0         1         0         0         0         0  \n",
       "1         0         1         0         0         0         0  \n",
       "2         0         1         0         0         0         0  \n",
       "\n",
       "[3 rows x 3073 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(\"## Merging ECFP Descriptors\"))\n",
    "\n",
    "# Drop duplicates just in case\n",
    "ecfp0 = ecfp0.drop_duplicates(subset=[\"mol_idx\"])\n",
    "ecfp2 = ecfp2.drop_duplicates(subset=[\"mol_idx\"])\n",
    "ecfp4 = ecfp4.drop_duplicates(subset=[\"mol_idx\"])\n",
    "\n",
    "# Merge all ECFP descriptors side-by-side\n",
    "ecfp_merged = ecfp0.merge(ecfp2, on=\"mol_idx\", suffixes=(\"_r0\", \"_r2\")).merge(ecfp4, on=\"mol_idx\", suffixes=(\"\", \"_r4\"))\n",
    "\n",
    "print(f\" ECFP merged shape: {ecfp_merged.shape}\")\n",
    "display(ecfp_merged.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f96b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignments columns:\n",
      "['mol_idx', 'atom_index', 'element', 'shift_ppm']\n",
      "\n",
      "HOSE columns:\n",
      "['mol_idx', 'atom_index', 'element', 'shift_ppm', 'hose_1', 'hose_2', 'hose_3', 'hose_4']\n"
     ]
    }
   ],
   "source": [
    "print(\"Assignments columns:\")\n",
    "print(assigns.columns.tolist())\n",
    "\n",
    "print(\"\\nHOSE columns:\")\n",
    "print(hose.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae148bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Chunk-Based Safe ECFP Compression (No Memory Errors)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Columns to use: 129 / 1025 total\n",
      " Compact ECFP saved at: C:\\DOCTORAL HUB\\nmr_pipeline_project\\data\\descriptors\\ecfp_compact.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"## Chunk-Based Safe ECFP Compression (No Memory Errors)\"))\n",
    "\n",
    "# Path to your large ECFP descriptor file\n",
    "ecfp_path = Path(ROOT / \"data/descriptors/ecfp0.csv\")  # or ecfp2.csv / ecfp4.csv if you prefer\n",
    "\n",
    "# Output path for reduced descriptor file\n",
    "compact_path = Path(ROOT / \"data/descriptors/ecfp_compact.csv\")\n",
    "\n",
    "# Define how many fingerprint columns to keep\n",
    "N_COLS_TO_KEEP = 128\n",
    "CHUNKSIZE = 5000  # number of rows processed at once\n",
    "\n",
    "# Read only the header to identify columns\n",
    "with open(ecfp_path, 'r') as f:\n",
    "    header = f.readline().strip().split(',')\n",
    "\n",
    "# Always keep mol_idx + first N_COLS_TO_KEEP fingerprint columns\n",
    "cols_to_use = ['mol_idx'] + header[1:N_COLS_TO_KEEP + 1]\n",
    "\n",
    "print(f\" Columns to use: {len(cols_to_use)} / {len(header)} total\")\n",
    "\n",
    "# Prepare the output file\n",
    "pd.DataFrame(columns=cols_to_use).to_csv(compact_path, index=False)\n",
    "\n",
    "# Process file in chunks to avoid memory overflow\n",
    "for chunk in pd.read_csv(ecfp_path, usecols=cols_to_use, chunksize=CHUNKSIZE):\n",
    "    chunk.to_csv(compact_path, mode='a', header=False, index=False)\n",
    "\n",
    "print(f\" Compact ECFP saved at: {compact_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc24f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ROOT path set to: C:\\DOCTORAL HUB\\nmr_pipeline_project\n",
      "✅ assignments_verified.csv -> C:\\DOCTORAL HUB\\nmr_pipeline_project\\data\\curated\\verified\\assignments_verified.csv\n",
      "✅ hose.csv -> C:\\DOCTORAL HUB\\nmr_pipeline_project\\data\\descriptors\\hose.csv\n",
      "✅ ecfp_compact.csv -> C:\\DOCTORAL HUB\\nmr_pipeline_project\\data\\descriptors\\ecfp_compact.csv\n",
      "\n",
      "✅ Assignments shape: (678775, 4)\n",
      "✅ HOSE shape: (513880, 8)\n",
      "✅ ECFP shape: (60792, 129)\n"
     ]
    }
   ],
   "source": [
    "# --- Load all necessary data for Phase 3 (Final Fixed CSV Version) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define project root\n",
    "ROOT = Path(r\"C:\\DOCTORAL HUB\\nmr_pipeline_project\")\n",
    "print(f\" ROOT path set to: {ROOT.resolve()}\")\n",
    "\n",
    "# Define correct CSV paths\n",
    "assign_path = ROOT / \"data/curated/verified/assignments_verified.csv\"\n",
    "hose_path = ROOT / \"data/descriptors/hose.csv\"\n",
    "ecfp_path = ROOT / \"data/descriptors/ecfp_compact.csv\"\n",
    "\n",
    "# Check if files exist before reading\n",
    "for path in [assign_path, hose_path, ecfp_path]:\n",
    "    print(f\"{'' if path.exists() else '❌'} {path.name} -> {path}\")\n",
    "\n",
    "# Load data (all CSVs)\n",
    "assigns = pd.read_csv(assign_path)\n",
    "hose = pd.read_csv(hose_path)\n",
    "ecfp_small = pd.read_csv(ecfp_path)\n",
    "\n",
    "print(f\"\\n Assignments shape: {assigns.shape}\")\n",
    "print(f\" HOSE shape: {hose.shape}\")\n",
    "print(f\" ECFP shape: {ecfp_small.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6466a4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merging HOSE (atom-level) with Assignments...\n",
      " After HOSE merge: (756690, 10)\n",
      "\n",
      " Preparing compact ECFP data...\n",
      " Compact ECFP prepared: (60792, 129)\n",
      "\n",
      " Starting streamed merge in chunks...\n",
      " Merged chunk 1/38 (20000 rows)\n",
      " Merged chunk 2/38 (20000 rows)\n",
      " Merged chunk 3/38 (20000 rows)\n",
      " Merged chunk 4/38 (20000 rows)\n",
      " Merged chunk 5/38 (20000 rows)\n",
      " Merged chunk 6/38 (20000 rows)\n",
      " Merged chunk 7/38 (20000 rows)\n",
      " Merged chunk 8/38 (20000 rows)\n",
      " Merged chunk 9/38 (20000 rows)\n",
      " Merged chunk 10/38 (20000 rows)\n",
      " Merged chunk 11/38 (20000 rows)\n",
      " Merged chunk 12/38 (20000 rows)\n",
      " Merged chunk 13/38 (20000 rows)\n",
      " Merged chunk 14/38 (20000 rows)\n",
      " Merged chunk 15/38 (20000 rows)\n",
      " Merged chunk 16/38 (20000 rows)\n",
      " Merged chunk 17/38 (20000 rows)\n",
      " Merged chunk 18/38 (20000 rows)\n",
      " Merged chunk 19/38 (20000 rows)\n",
      " Merged chunk 20/38 (20000 rows)\n",
      " Merged chunk 21/38 (20000 rows)\n",
      " Merged chunk 22/38 (20000 rows)\n",
      " Merged chunk 23/38 (20000 rows)\n",
      " Merged chunk 24/38 (20000 rows)\n",
      " Merged chunk 25/38 (20000 rows)\n",
      " Merged chunk 26/38 (20000 rows)\n",
      " Merged chunk 27/38 (20000 rows)\n",
      " Merged chunk 28/38 (20000 rows)\n",
      " Merged chunk 29/38 (20000 rows)\n",
      " Merged chunk 30/38 (20000 rows)\n",
      " Merged chunk 31/38 (20000 rows)\n",
      " Merged chunk 32/38 (20000 rows)\n",
      " Merged chunk 33/38 (20000 rows)\n",
      " Merged chunk 34/38 (20000 rows)\n",
      " Merged chunk 35/38 (20000 rows)\n",
      " Merged chunk 36/38 (20000 rows)\n",
      " Merged chunk 37/38 (20000 rows)\n",
      " Merged chunk 38/38 (16690 rows)\n",
      "\n",
      " All chunks merged successfully!\n",
      " Final saved file: C:\\DOCTORAL HUB\\nmr_pipeline_project\\data\\merged\\merged_phase3.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 3 Final Merge: Streamed Mode (Memory-Safe) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define output folder and file\n",
    "output_path = ROOT / \"data/merged/merged_phase3.csv\"\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Step 1: Merge HOSE (atom-level) with Assignments ---\n",
    "print(\" Merging HOSE (atom-level) with Assignments...\")\n",
    "merged = assigns.merge(hose, on=[\"mol_idx\", \"atom_index\"], how=\"inner\")\n",
    "print(f\" After HOSE merge: {merged.shape}\")\n",
    "\n",
    "# --- Step 2: Prepare ECFP data ---\n",
    "print(\"\\n Preparing compact ECFP data...\")\n",
    "ecfp_small = ecfp_small.drop_duplicates(subset=[\"mol_idx\"]).reset_index(drop=True)\n",
    "ecfp_small = ecfp_small.fillna(0)\n",
    "print(f\" Compact ECFP prepared: {ecfp_small.shape}\")\n",
    "\n",
    "# --- Step 3: Streamed merge in chunks to avoid memory overflow ---\n",
    "print(\"\\n Starting streamed merge in chunks...\")\n",
    "CHUNK = 20000  # adjust if needed\n",
    "cols = list(merged.columns) + list(ecfp_small.columns[1:])\n",
    "\n",
    "# Write header only once\n",
    "pd.DataFrame(columns=cols).to_csv(output_path, index=False)\n",
    "\n",
    "# Merge and append chunk-by-chunk\n",
    "for i in range(0, len(merged), CHUNK):\n",
    "    part = merged.iloc[i:i+CHUNK].merge(ecfp_small, on=\"mol_idx\", how=\"left\")\n",
    "    part.to_csv(output_path, mode=\"a\", header=False, index=False)\n",
    "    print(f\" Merged chunk {i//CHUNK + 1}/{(len(merged)//CHUNK) + 1} ({len(part)} rows)\")\n",
    "\n",
    "print(f\"\\n All chunks merged successfully!\\n Final saved file: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84d2b413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Verifying Phase 3 Merged Data Integrity...\n",
      "\n",
      "Assignments: (678775, 4)\n",
      "HOSE: (513880, 8)\n",
      "ECFP (Compact): (60792, 129)\n",
      "Merged: (756690, 138)\n",
      "\n",
      " Molecule Counts\n",
      "Assignments mol_idx: 61215\n",
      "HOSE mol_idx:        49761\n",
      "ECFP mol_idx:        60792\n",
      "Merged mol_idx:      49761\n",
      "\n",
      " Atom Counts\n",
      "Assignments atoms: 678775\n",
      "HOSE atoms:        513880\n",
      "Merged atoms:      756690\n",
      "\n",
      " Data Quality Checks\n",
      "Missing values: 0\n",
      "Duplicate (mol_idx + atom_index): 322543\n",
      "\n",
      " Molecule count preserved.\n",
      " Atom-level mapping nearly complete.\n",
      " No missing values detected.\n",
      " 322543 duplicate atom entries found — verify consistency.\n",
      "\n",
      " Integrity verification complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 3 Verification: Data Integrity & Consistency ---\n",
    "import pandas as pd\n",
    "\n",
    "print(\" Verifying Phase 3 Merged Data Integrity...\\n\")\n",
    "\n",
    "# Load the merged dataset (from the output file we created)\n",
    "merged_path = ROOT / \"data/merged/merged_phase3.csv\"\n",
    "merged_df = pd.read_csv(merged_path)\n",
    "\n",
    "# --- Basic Shapes ---\n",
    "print(f\"Assignments: {assigns.shape}\")\n",
    "print(f\"HOSE: {hose.shape}\")\n",
    "print(f\"ECFP (Compact): {ecfp_small.shape}\")\n",
    "print(f\"Merged: {merged_df.shape}\")\n",
    "\n",
    "# --- Molecule and Atom Counts ---\n",
    "mol_in_assign = assigns['mol_idx'].nunique()\n",
    "mol_in_hose = hose['mol_idx'].nunique()\n",
    "mol_in_ecfp = ecfp_small['mol_idx'].nunique()\n",
    "mol_in_merged = merged_df['mol_idx'].nunique()\n",
    "\n",
    "atom_in_assign = assigns.shape[0]\n",
    "atom_in_hose = hose.shape[0]\n",
    "atom_in_merged = merged_df.shape[0]\n",
    "\n",
    "print(\"\\n Molecule Counts\")\n",
    "print(f\"Assignments mol_idx: {mol_in_assign}\")\n",
    "print(f\"HOSE mol_idx:        {mol_in_hose}\")\n",
    "print(f\"ECFP mol_idx:        {mol_in_ecfp}\")\n",
    "print(f\"Merged mol_idx:      {mol_in_merged}\")\n",
    "\n",
    "print(\"\\n Atom Counts\")\n",
    "print(f\"Assignments atoms: {atom_in_assign}\")\n",
    "print(f\"HOSE atoms:        {atom_in_hose}\")\n",
    "print(f\"Merged atoms:      {atom_in_merged}\")\n",
    "\n",
    "# --- Missing Value & Duplicate Checks ---\n",
    "missing_vals = merged_df.isnull().sum().sum()\n",
    "duplicates = merged_df.duplicated(subset=[\"mol_idx\", \"atom_index\"]).sum()\n",
    "\n",
    "print(\"\\n Data Quality Checks\")\n",
    "print(f\"Missing values: {missing_vals}\")\n",
    "print(f\"Duplicate (mol_idx + atom_index): {duplicates}\")\n",
    "\n",
    "# --- Sanity Assertions ---\n",
    "if mol_in_merged <= mol_in_ecfp:\n",
    "    print(\"\\n Molecule count preserved.\")\n",
    "else:\n",
    "    print(\"\\n Molecule mismatch detected!\")\n",
    "\n",
    "if atom_in_merged >= 0.95 * atom_in_assign:\n",
    "    print(\" Atom-level mapping nearly complete.\")\n",
    "else:\n",
    "    print(\" Potential loss in atom-level mapping — investigate.\")\n",
    "\n",
    "if missing_vals == 0:\n",
    "    print(\" No missing values detected.\")\n",
    "else:\n",
    "    print(\" Missing values exist — consider imputing or reviewing.\")\n",
    "\n",
    "if duplicates == 0:\n",
    "    print(\" No duplicate atom entries.\")\n",
    "else:\n",
    "    print(f\" {duplicates} duplicate atom entries found — verify consistency.\")\n",
    "\n",
    "print(\"\\n Integrity verification complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29c822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning up duplicate atom entries (mol_idx + atom_index)...\n",
      "\n",
      " Cleanup complete!\n",
      " Original merged rows: 756,690\n",
      " Cleaned merged rows: 434,147\n",
      " Duplicates removed:  322,543\n",
      " Molecules before:    49,761\n",
      " Molecules after:     49,761\n",
      "\n",
      " Molecule-level integrity preserved — no loss of chemical information.\n",
      "\n",
      " Cleaned dataset saved to:\n",
      "C:\\DOCTORAL HUB\\nmr_pipeline_project\\data\\merged\\merged_phase3_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 3 Cleanup: Remove duplicate atom entries (Safe + Verified) ---\n",
    "print(\" Cleaning up duplicate atom entries (mol_idx + atom_index)...\")\n",
    "\n",
    "# Track before cleanup\n",
    "before_rows = merged_df.shape[0]\n",
    "before_mols = merged_df[\"mol_idx\"].nunique()\n",
    "\n",
    "# Remove exact duplicates (same molecule + atom)\n",
    "merged_clean = merged_df.drop_duplicates(subset=[\"mol_idx\", \"atom_index\"], keep=\"first\")\n",
    "\n",
    "# Track after cleanup\n",
    "after_rows = merged_clean.shape[0]\n",
    "after_mols = merged_clean[\"mol_idx\"].nunique()\n",
    "\n",
    "# Save cleaned version safely\n",
    "clean_path = ROOT / \"data/merged/merged_phase3_clean.csv\"\n",
    "merged_clean.to_csv(clean_path, index=False)\n",
    "\n",
    "# --- Verification summary ---\n",
    "print(\"\\n Cleanup complete!\")\n",
    "print(f\" Original merged rows: {before_rows:,}\")\n",
    "print(f\" Cleaned merged rows: {after_rows:,}\")\n",
    "print(f\" Duplicates removed:  {before_rows - after_rows:,}\")\n",
    "print(f\" Molecules before:    {before_mols:,}\")\n",
    "print(f\" Molecules after:     {after_mols:,}\")\n",
    "\n",
    "if before_mols == after_mols:\n",
    "    print(\"\\n Molecule-level integrity preserved — no loss of chemical information.\")\n",
    "else:\n",
    "    print(\"\\n Warning: Molecule count changed — check mapping consistency.\")\n",
    "\n",
    "print(f\"\\n Cleaned dataset saved to:\\n{clean_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d0ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmr_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
